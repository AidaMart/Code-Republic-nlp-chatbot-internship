{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "* In contrast to stemming, lemmatization looks beyond word reduction, and considers a language's full vocabulary to apply a morphological analysis to words.\n",
    "* The lemma of 'was' is 'be' and the lemma of 'mice' is 'mouse'. Further, the lemma of 'meeting' might be 'meet' or 'meeting' depending on its use in a sentence.\n",
    "\n",
    "* Lemmatization is typically seen as much more informative than simple stemming, which is why Spacy has opted to only have Lemmatization available instead of stemming.\n",
    "\n",
    "* Lemmatization looks at surrounding text to determine a given word's part of speech, it does not categorize phrases. \n",
    "* Next we'll discuss word vectors and similarity.\n",
    "* Let's discuss how to perform lemmatization with Spacy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(u'I am runner running in a race because I love to run since I ran today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 4690420944186131903 \t I\n",
      "am \t AUX \t 10382539506755952630 \t be\n",
      "runner \t NOUN \t 12640964157389618806 \t runner\n",
      "running \t VERB \t 12767647472892411841 \t run\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "race \t NOUN \t 8048469955494714898 \t race\n",
      "because \t SCONJ \t 16950148841647037698 \t because\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "love \t VERB \t 3702023516439754181 \t love\n",
      "to \t PART \t 3791531372978436496 \t to\n",
      "run \t VERB \t 12767647472892411841 \t run\n",
      "since \t SCONJ \t 10066841407251338481 \t since\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "ran \t VERB \t 12767647472892411841 \t run\n",
      "today \t NOUN \t 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u'I saw ten mice today!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903    I\n",
      "saw          VERB   11925638236994514241   see\n",
      "ten          NUM    7970704286052693043    ten\n",
      "mice         NOUN   1384165645700560590    mouse\n",
      "today        NOUN   11042482332948150395   today\n",
      "!            PUNCT  17494803046312582752   !\n"
     ]
    }
   ],
   "source": [
    "show_lemmas(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words\n",
    "\n",
    "* Words lika \"a\" and \"the\" appear so frequently that they don't require tagging as thoroghly as nouns, verbs and modifiers.\n",
    "* We call these stop words, and they can be filtered from the text to be processed.\n",
    "* Spacy holds a built-in list of some 305 English stop words.\n",
    "\n",
    "* Sometimes stop words will hurt to our natural language processing, so they just the really common words and that don't give us additional information, and we must remove stop  words. And we will discuss how to remove stop words with spaCy, and how to add our own special stop words in case maybe you're dealing with a unique data set where you have an acronym that gets used too much and you want to add that as a stop word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'along', 'someone', 'part', 'everyone', 'through', 'does', 'ten', '’d', 'mine', 'first', 'sixty', 'third', 'together', 'noone', 'the', 'their', 'several', 'already', 'around', 'could', 'be', 'n‘t', 'before', 'yet', 'call', 'same', 'my', 'has', 'never', 'put', 'see', \"'re\", 'here', 'rather', 'them', 'n’t', 'unless', 'are', '‘s', 'by', 'too', 'therein', 'give', 'whereafter', 'those', 'still', 'would', 'bottom', 'these', 'under', 'wherever', 'fifteen', 'keep', 'becomes', 'who', 'nothing', 'since', 'four', 'whole', 'and', 'almost', 'beyond', 'used', 'twenty', 'may', 'well', 'ca', 'you', 'thereby', 'might', 'though', 'across', 'his', 'within', 'herself', 'was', 'even', 're', 'whether', 'whereby', 'after', 'while', 'in', 'nine', 'about', 'now', \"'s\", 'both', 'an', 'she', 'on', 'always', 'whither', 'herein', 'yours', 'seemed', 'whom', 'moreover', 'one', 'did', 'somewhere', 'own', 'above', 'hundred', 'whoever', 'it', 'itself', 'name', 'otherwise', 'this', 'least', 'either', 'being', 'yourselves', 'elsewhere', 'please', 'they', 'until', 'for', 'into', 'make', 'whatever', 'there', 'such', 'anything', 'have', 'himself', 'themselves', 'ourselves', 'all', 'further', 'thereupon', 'off', 'enough', 'many', 'go', '’ve', 'hers', 'say', 'take', 'thus', 'seeming', 'am', 'various', 'any', 'me', 'of', 'latter', 'mostly', 'without', 'where', 'former', \"n't\", 'more', 'with', 'eleven', 'thru', 'anyhow', '‘ll', \"'d\", 'eight', 'toward', 'really', 'between', 'behind', 'doing', '‘re', 'he', 'made', 'some', 'towards', 'whenever', 'is', 'something', 'three', 'formerly', 'due', 'indeed', 'whose', 'somehow', 'throughout', 'its', 'most', 'upon', 'back', 'every', 'hereafter', 'will', 'afterwards', 'your', 'besides', 'again', 'per', 'when', 'whereupon', 'top', 'so', 'amount', 'out', 'had', '‘ve', 'not', '’re', 'each', '’ll', 'latterly', 'neither', 'also', 'hereby', 'which', 'hence', 'another', 'once', 'during', 'quite', '‘m', 'do', 'just', 'often', 'twelve', \"'ve\", 'regarding', 'less', 'becoming', 'anywhere', 'only', 'at', 'that', 'serious', 'whereas', 'her', 'although', 'below', 'onto', 'why', '’m', 'nowhere', 'as', 'should', 'then', 'or', 'how', 'thence', 'sometimes', 'wherein', 'whence', 'beforehand', 'i', 'meanwhile', 'ours', 'few', 'no', 'except', 'become', 'we', 'namely', 'against', 'last', \"'m\", 'anyway', 'but', 'side', 'among', 'perhaps', 'thereafter', 'next', 'must', 'to', 'six', 'everything', 'beside', 'two', 'five', 'nor', 'anyone', 'hereupon', 'full', 'none', 'down', 'show', 'what', 'amongst', 'everywhere', 'cannot', 'other', 'us', 'if', 'move', 'others', 'became', 'nobody', 'were', 'him', 'myself', 'yourself', 'sometime', 'therefore', '’s', 'seem', 'much', 'front', 'alone', 'get', 'our', 'a', 'seems', 'forty', \"'ll\", 'nevertheless', 'over', 'than', 'from', 'empty', 'very', 'else', 'can', 'using', 'up', 'been', 'because', 'via', 'ever', 'however', 'fifty', '‘d', 'done'}\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['is'].is_stop\n",
    "nlp.vocab['mystery'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.add('btw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['btw'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)\n",
    "nlp.vocab['btw'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.remove('beyond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['beyond'].is_stop= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['beyond'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary and Matching \n",
    "\n",
    "* We've seen how a body of text is divided into tokens, and how individual tokens are parsed and tagged with parts of speech, dependencies and lemmas.\n",
    "* We will identify and label specific phrases that match patterns we can define ourselves.\n",
    "\n",
    "* We can say this is a powerful version of Regular Expression where we actually take parts of speech into account for our pattern search.\n",
    "\n",
    "Compared to using regular expressions on raw text, spaCy’s rule-based matcher engines and components not only let you find the words and phrases you’re looking for – they also give you access to the tokens within the document and their relationships. This means you can easily access and analyze the surrounding tokens, merge spans into single tokens or add entries to the named entities in doc.ents.\n",
    "\n",
    "# Token based matching\n",
    "\n",
    "Orth (short for \"orthography\"): while text refers to the actual words and characters of a document, orth refers to the standardized spelling and writing conventions used for a language, and is often used to refer to the normalized form of text after pre-processing.\n",
    "\n",
    "Text: The original text of the token.\n",
    "\n",
    "Lower: The lowercase form of the token.\n",
    "\n",
    "Lemma: The base or dictionary form of the token.\n",
    "\n",
    "Pos: The part-of-speech (POS) tag of the token, which identifies the grammatical role of the word in the sentence.\n",
    "\n",
    "Tag: The detailed POS tag of the token, which provides additional information about the word, such as tense or number.\n",
    "\n",
    "Dep: The dependency label of the token, which describes the syntactic relationship between the word and its dependents.\n",
    "\n",
    "Shape: The shape of the token, which captures its capitalization, punctuation, and other formatting.\n",
    "\n",
    "is_alpha: A Boolean attribute that indicates whether the token consists entirely of alphabetic characters.\n",
    "\n",
    "is_digit: A Boolean attribute that indicates whether the token consists entirely of digits.\n",
    "\n",
    "is_punct: A Boolean attribute that indicates whether the token is a punctuation mark.\n",
    "\n",
    "is_space: A Boolean attribute that indicates whether the token is a whitespace character.\n",
    "\n",
    "is_stop: A Boolean attribute that indicates whether the token is a stop word, which is a common word that is often excluded from analysis.\n",
    "\n",
    "OP: operator or quantifier to determine how often to match a token pattern. \n",
    "\n",
    "* !\tNegate the pattern, by requiring it to match exactly 0 times.\n",
    "\n",
    "* ?\tMake the pattern optional, by allowing it to match 0 or 1 times.\n",
    "\n",
    "* '+' Require the pattern to match 1 or more times.\n",
    "\n",
    "* '*' Allow the pattern to match zero or more times.\n",
    "\n",
    "* {n}\tRequire the pattern to match exactly n times.\n",
    "\n",
    "* {n,m}\tRequire the pattern to match at least n but not more than m times.\n",
    "\n",
    "* {n,}\tRequire the pattern to match at least n times.\n",
    "\n",
    "* {,m}\tRequire the pattern to match at most m times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule based matching\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"The use of blockchain-based smart contracts has greatly reduced transaction costs in the renewable energy sector.\"\n",
    "          \"As more companies adopt blockchain based solutions for energy management, we can expect to see increased efficiency and transparency in the industry of Block Chain Based\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SolarPower\n",
    "pattern1 = [{'LOWER': 'blockchain'}, {'LOWER': 'based'}]\n",
    "# Solar-power\n",
    "pattern2 = [{'LOWER': 'blockchain'}, {'IS_PUNCT' : True}, {'LOWER': 'based'}]\n",
    "# Solar power\n",
    "pattern3 = [{'LOWER' : 'block'}, {'LOWER' : 'chain' }, {'LOWER': 'based'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('Blockchain-based', patterns=[pattern1, pattern2, pattern3], on_match=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12507262609179161830, 3, 6), (12507262609179161830, 23, 25), (12507262609179161830, 43, 46)]\n"
     ]
    }
   ],
   "source": [
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12507262609179161830 Blockchain-based 3 6 blockchain-based\n",
      "12507262609179161830 Blockchain-based 23 25 blockchain based\n",
      "12507262609179161830 Blockchain-based 43 46 Block Chain Based\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[match_id] # get string representation\n",
    "    span = doc[start:end]                # get the matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.remove('Blockchain-based')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = [{'LOWER' : 'blockchain'}, {'LOWER' : 'based'}]\n",
    "#solar.power and more\n",
    "pattern2 = [{'LOWER' : 'blockchain'}, {'IS_PUNCT' : True, 'OP': '*' }, {'LOWER' : 'based' }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add() takes at least 2 positional arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m matcher\u001b[39m.\u001b[39;49madd(\u001b[39m'\u001b[39;49m\u001b[39mBlockchain-based\u001b[39;49m\u001b[39m'\u001b[39;49m, patterns\u001b[39m=\u001b[39;49m[pattern1, pattern2], on_match\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/spacy/matcher/phrasematcher.pyx:158\u001b[0m, in \u001b[0;36mspacy.matcher.phrasematcher.PhraseMatcher.add\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: add() takes at least 2 positional arguments (1 given)"
     ]
    }
   ],
   "source": [
    "matcher.add('Blockchain-based', patterns=[pattern1, pattern2], on_match=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"Blockchain--based is solarpower !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches = matcher(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12507262609179161830, 0, 3)]\n"
     ]
    }
   ],
   "source": [
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part two -> phrase matching\n",
    "\n",
    "In previous we saw how to performe rule-based matching. In alternative and often more efficient method is to actually match on a terminology list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../TextFiles/reaganomics.txt') as fs:\n",
    "    doc3 = nlp(fs.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "phrase_patterns = [nlp(text) for text in phrase_list]\n",
    "print(type(phrase_patterns[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('EconMatcher', phrase_patterns, on_match=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches = matcher(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3680293220734633682, 41, 45), (3680293220734633682, 49, 53), (3680293220734633682, 54, 56), (3680293220734633682, 61, 65), (3680293220734633682, 673, 677), (3680293220734633682, 2986, 2990)]\n"
     ]
    }
   ],
   "source": [
    "print(found_matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680293220734633682 EconMatcher 41 45 supply-side economics\n",
      "3680293220734633682 EconMatcher 49 53 trickle-down economics\n",
      "3680293220734633682 EconMatcher 54 56 voodoo economics\n",
      "3680293220734633682 EconMatcher 61 65 free-market economics\n",
      "3680293220734633682 EconMatcher 673 677 supply-side economics\n",
      "3680293220734633682 EconMatcher 2986 2990 trickle-down economics\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc3[start: end]\n",
    "    #span = doc3[start - 5: end + 5]\n",
    "\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
