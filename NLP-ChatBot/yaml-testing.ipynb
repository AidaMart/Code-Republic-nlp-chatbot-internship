{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1806e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6853566342880726112\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 787013855824534352\n",
      "physical_device_desc: \"device: 0, name: METAL, pci bus id: <undefined>\"\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 12:11:04.024294: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2023-07-08 12:11:04.024309: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2023-07-08 12:11:04.024312: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2023-07-08 12:11:04.024514: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-07-08 12:11:04.024526: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "999c95f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 12:11:08.657977: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-07-08 12:11:08.658070: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378d98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "440291ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, 38)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)        [(None, 563)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)     (None, 38, 200)              1430000   ['input_5[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)     (None, 563, 200)             1430000   ['input_6[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               [(None, 200),                320800    ['embedding_2[0][0]']         \n",
      "                              (None, 200),                                                        \n",
      "                              (None, 200)]                                                        \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               [(None, 563, 200),           320800    ['embedding_3[0][0]',         \n",
      "                              (None, 200),                           'lstm_2[0][1]',              \n",
      "                              (None, 200)]                           'lstm_2[0][2]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 563, 7150)            1437150   ['lstm_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4938750 (18.84 MB)\n",
      "Trainable params: 4938750 (18.84 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 13:05:26.270861: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-08 13:05:26.630279: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-08 13:05:26.813665: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-08 13:05:28.149176: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1/147 [..............................] - ETA: 15:03 - loss: 8.8748 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 13:05:30.851373: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 46s 271ms/step - loss: 7.1508 - accuracy: 0.0509\n",
      "Epoch 2/300\n",
      "147/147 [==============================] - 38s 262ms/step - loss: 6.3871 - accuracy: 0.0671\n",
      "Epoch 3/300\n",
      "147/147 [==============================] - 39s 268ms/step - loss: 6.1779 - accuracy: 0.0903\n",
      "Epoch 4/300\n",
      "147/147 [==============================] - 41s 278ms/step - loss: 6.0124 - accuracy: 0.0989\n",
      "Epoch 5/300\n",
      "147/147 [==============================] - 39s 266ms/step - loss: 5.8552 - accuracy: 0.1074\n",
      "Epoch 6/300\n",
      "147/147 [==============================] - 39s 266ms/step - loss: 5.6971 - accuracy: 0.1208\n",
      "Epoch 7/300\n",
      "147/147 [==============================] - 39s 266ms/step - loss: 5.5396 - accuracy: 0.1351\n",
      "Epoch 8/300\n",
      "147/147 [==============================] - 39s 265ms/step - loss: 5.3790 - accuracy: 0.1505\n",
      "Epoch 9/300\n",
      "147/147 [==============================] - 40s 271ms/step - loss: 5.2221 - accuracy: 0.1619\n",
      "Epoch 10/300\n",
      "147/147 [==============================] - 39s 267ms/step - loss: 5.0747 - accuracy: 0.1733\n",
      "Epoch 11/300\n",
      "147/147 [==============================] - 39s 266ms/step - loss: 4.9271 - accuracy: 0.1838\n",
      "Epoch 12/300\n",
      "147/147 [==============================] - 39s 264ms/step - loss: 4.7889 - accuracy: 0.1931\n",
      "Epoch 13/300\n",
      "147/147 [==============================] - 39s 266ms/step - loss: 4.6562 - accuracy: 0.2036\n",
      "Epoch 14/300\n",
      "147/147 [==============================] - 41s 278ms/step - loss: 4.5208 - accuracy: 0.2127\n",
      "Epoch 15/300\n",
      "147/147 [==============================] - 39s 268ms/step - loss: 4.3899 - accuracy: 0.2214\n",
      "Epoch 16/300\n",
      "147/147 [==============================] - 39s 266ms/step - loss: 4.2653 - accuracy: 0.2298\n",
      "Epoch 17/300\n",
      "147/147 [==============================] - 41s 279ms/step - loss: 4.1586 - accuracy: 0.2373\n",
      "Epoch 18/300\n",
      "147/147 [==============================] - 41s 276ms/step - loss: 4.0417 - accuracy: 0.2463\n",
      "Epoch 19/300\n",
      "147/147 [==============================] - 40s 271ms/step - loss: 3.9231 - accuracy: 0.2546\n",
      "Epoch 20/300\n",
      "147/147 [==============================] - 40s 271ms/step - loss: 3.8062 - accuracy: 0.2644\n",
      "Epoch 21/300\n",
      "147/147 [==============================] - 40s 272ms/step - loss: 3.6984 - accuracy: 0.2735\n",
      "Epoch 22/300\n",
      "147/147 [==============================] - 40s 273ms/step - loss: 3.5952 - accuracy: 0.2818\n",
      "Epoch 23/300\n",
      "147/147 [==============================] - 40s 274ms/step - loss: 3.4931 - accuracy: 0.2910\n",
      "Epoch 24/300\n",
      "147/147 [==============================] - 41s 275ms/step - loss: 3.3972 - accuracy: 0.2993\n",
      "Epoch 25/300\n",
      "147/147 [==============================] - 41s 276ms/step - loss: 3.3053 - accuracy: 0.3074\n",
      "Epoch 26/300\n",
      "147/147 [==============================] - 41s 278ms/step - loss: 3.2171 - accuracy: 0.3160\n",
      "Epoch 27/300\n",
      "147/147 [==============================] - 41s 277ms/step - loss: 3.1344 - accuracy: 0.3247\n",
      "Epoch 28/300\n",
      "147/147 [==============================] - 41s 279ms/step - loss: 3.0541 - accuracy: 0.3328\n",
      "Epoch 29/300\n",
      "147/147 [==============================] - 41s 279ms/step - loss: 2.9794 - accuracy: 0.3422\n",
      "Epoch 30/300\n",
      "147/147 [==============================] - 41s 279ms/step - loss: 2.9136 - accuracy: 0.3499\n",
      "Epoch 31/300\n",
      "147/147 [==============================] - 40s 274ms/step - loss: 2.8378 - accuracy: 0.3596\n",
      "Epoch 32/300\n",
      "147/147 [==============================] - 41s 279ms/step - loss: 2.7600 - accuracy: 0.3704\n",
      "Epoch 33/300\n",
      "147/147 [==============================] - 42s 287ms/step - loss: 2.6908 - accuracy: 0.3789\n",
      "Epoch 34/300\n",
      "147/147 [==============================] - 42s 286ms/step - loss: 2.6231 - accuracy: 0.3881\n",
      "Epoch 35/300\n",
      "147/147 [==============================] - 42s 287ms/step - loss: 2.5624 - accuracy: 0.3958\n",
      "Epoch 36/300\n",
      "147/147 [==============================] - 42s 285ms/step - loss: 2.4970 - accuracy: 0.4045\n",
      "Epoch 37/300\n",
      "147/147 [==============================] - 42s 287ms/step - loss: 2.4340 - accuracy: 0.4134\n",
      "Epoch 38/300\n",
      "147/147 [==============================] - 42s 288ms/step - loss: 2.3789 - accuracy: 0.4207\n",
      "Epoch 39/300\n",
      "147/147 [==============================] - 42s 286ms/step - loss: 2.3239 - accuracy: 0.4283\n",
      "Epoch 40/300\n",
      "147/147 [==============================] - 42s 285ms/step - loss: 2.2671 - accuracy: 0.4374\n",
      "Epoch 41/300\n",
      "147/147 [==============================] - 42s 288ms/step - loss: 2.2129 - accuracy: 0.4447\n",
      "Epoch 42/300\n",
      "147/147 [==============================] - 42s 284ms/step - loss: 2.1625 - accuracy: 0.4515\n",
      "Epoch 43/300\n",
      "147/147 [==============================] - 42s 284ms/step - loss: 2.1129 - accuracy: 0.4592\n",
      "Epoch 44/300\n",
      "147/147 [==============================] - 41s 282ms/step - loss: 2.0673 - accuracy: 0.4669\n",
      "Epoch 45/300\n",
      "147/147 [==============================] - 41s 280ms/step - loss: 2.0271 - accuracy: 0.4734\n",
      "Epoch 46/300\n",
      "147/147 [==============================] - 41s 281ms/step - loss: 1.9819 - accuracy: 0.4806\n",
      "Epoch 47/300\n",
      "147/147 [==============================] - 41s 282ms/step - loss: 1.9428 - accuracy: 0.4869\n",
      "Epoch 48/300\n",
      "147/147 [==============================] - 43s 292ms/step - loss: 1.9047 - accuracy: 0.4933\n",
      "Epoch 49/300\n",
      "147/147 [==============================] - 42s 283ms/step - loss: 1.8639 - accuracy: 0.5005\n",
      "Epoch 50/300\n",
      "147/147 [==============================] - 41s 281ms/step - loss: 1.8250 - accuracy: 0.5071\n",
      "Epoch 51/300\n",
      "147/147 [==============================] - 45s 306ms/step - loss: 1.7854 - accuracy: 0.5139\n",
      "Epoch 52/300\n",
      "147/147 [==============================] - 42s 285ms/step - loss: 1.7489 - accuracy: 0.5215\n",
      "Epoch 53/300\n",
      "147/147 [==============================] - 42s 285ms/step - loss: 1.7134 - accuracy: 0.5282\n",
      "Epoch 54/300\n",
      "147/147 [==============================] - 44s 301ms/step - loss: 1.6806 - accuracy: 0.5335\n",
      "Epoch 55/300\n",
      "147/147 [==============================] - 43s 294ms/step - loss: 1.6459 - accuracy: 0.5406\n",
      "Epoch 56/300\n",
      "147/147 [==============================] - 43s 294ms/step - loss: 1.6150 - accuracy: 0.5462\n",
      "Epoch 57/300\n",
      "147/147 [==============================] - 44s 300ms/step - loss: 1.5871 - accuracy: 0.5514\n",
      "Epoch 58/300\n",
      "147/147 [==============================] - 43s 291ms/step - loss: 1.5586 - accuracy: 0.5576\n",
      "Epoch 59/300\n",
      "147/147 [==============================] - 43s 292ms/step - loss: 1.5271 - accuracy: 0.5632\n",
      "Epoch 60/300\n",
      "147/147 [==============================] - 43s 290ms/step - loss: 1.4963 - accuracy: 0.5699\n",
      "Epoch 61/300\n",
      "147/147 [==============================] - 42s 289ms/step - loss: 1.4670 - accuracy: 0.5761\n",
      "Epoch 62/300\n",
      "147/147 [==============================] - 42s 287ms/step - loss: 1.4382 - accuracy: 0.5821\n",
      "Epoch 63/300\n",
      "147/147 [==============================] - 43s 291ms/step - loss: 1.4124 - accuracy: 0.5871\n",
      "Epoch 64/300\n",
      "147/147 [==============================] - 43s 295ms/step - loss: 1.3845 - accuracy: 0.5944\n",
      "Epoch 65/300\n",
      "147/147 [==============================] - 43s 296ms/step - loss: 1.3619 - accuracy: 0.5993\n",
      "Epoch 66/300\n",
      "147/147 [==============================] - 43s 290ms/step - loss: 1.3424 - accuracy: 0.6030\n",
      "Epoch 67/300\n",
      "147/147 [==============================] - 45s 307ms/step - loss: 1.3209 - accuracy: 0.6086\n",
      "Epoch 68/300\n",
      "147/147 [==============================] - 44s 297ms/step - loss: 1.2982 - accuracy: 0.6128\n",
      "Epoch 69/300\n",
      "147/147 [==============================] - 44s 301ms/step - loss: 1.2739 - accuracy: 0.6181\n",
      "Epoch 70/300\n",
      "147/147 [==============================] - 53s 363ms/step - loss: 1.2564 - accuracy: 0.6237\n",
      "Epoch 71/300\n",
      "147/147 [==============================] - 46s 310ms/step - loss: 1.2380 - accuracy: 0.6282\n",
      "Epoch 72/300\n",
      "147/147 [==============================] - 46s 314ms/step - loss: 1.2169 - accuracy: 0.6326\n",
      "Epoch 73/300\n",
      "147/147 [==============================] - 42s 289ms/step - loss: 1.1981 - accuracy: 0.6366\n",
      "Epoch 74/300\n",
      "147/147 [==============================] - 42s 286ms/step - loss: 1.1803 - accuracy: 0.6417\n",
      "Epoch 75/300\n",
      "147/147 [==============================] - 42s 287ms/step - loss: 1.1649 - accuracy: 0.6460\n",
      "Epoch 76/300\n",
      "147/147 [==============================] - 42s 288ms/step - loss: 1.1487 - accuracy: 0.6494\n",
      "Epoch 77/300\n",
      "147/147 [==============================] - 41s 279ms/step - loss: 1.1288 - accuracy: 0.6546\n",
      "Epoch 78/300\n",
      "147/147 [==============================] - 44s 300ms/step - loss: 1.1103 - accuracy: 0.6597\n",
      "Epoch 79/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 44s 300ms/step - loss: 1.0978 - accuracy: 0.6632\n",
      "Epoch 80/300\n",
      "147/147 [==============================] - 43s 293ms/step - loss: 1.0793 - accuracy: 0.6686\n",
      "Epoch 81/300\n",
      "147/147 [==============================] - 44s 297ms/step - loss: 1.0606 - accuracy: 0.6741\n",
      "Epoch 82/300\n",
      "147/147 [==============================] - 44s 298ms/step - loss: 1.0405 - accuracy: 0.6806\n",
      "Epoch 83/300\n",
      "147/147 [==============================] - 45s 309ms/step - loss: 1.0273 - accuracy: 0.6831\n",
      "Epoch 84/300\n",
      "147/147 [==============================] - 44s 300ms/step - loss: 1.0157 - accuracy: 0.6880\n",
      "Epoch 85/300\n",
      "147/147 [==============================] - 46s 312ms/step - loss: 0.9976 - accuracy: 0.6927\n",
      "Epoch 86/300\n",
      "147/147 [==============================] - 44s 297ms/step - loss: 0.9821 - accuracy: 0.6974\n",
      "Epoch 87/300\n",
      "147/147 [==============================] - 43s 294ms/step - loss: 0.9694 - accuracy: 0.7013\n",
      "Epoch 88/300\n",
      "147/147 [==============================] - 43s 294ms/step - loss: 0.9564 - accuracy: 0.7050\n",
      "Epoch 89/300\n",
      "147/147 [==============================] - 43s 290ms/step - loss: 0.9418 - accuracy: 0.7105\n",
      "Epoch 90/300\n",
      "147/147 [==============================] - 43s 291ms/step - loss: 0.9335 - accuracy: 0.7126\n",
      "Epoch 91/300\n",
      "147/147 [==============================] - 43s 291ms/step - loss: 0.9170 - accuracy: 0.7170\n",
      "Epoch 92/300\n",
      "147/147 [==============================] - 43s 290ms/step - loss: 0.9034 - accuracy: 0.7223\n",
      "Epoch 93/300\n",
      "147/147 [==============================] - 43s 291ms/step - loss: 0.8925 - accuracy: 0.7266\n",
      "Epoch 94/300\n",
      "147/147 [==============================] - 43s 291ms/step - loss: 0.8808 - accuracy: 0.7288\n",
      "Epoch 95/300\n",
      "147/147 [==============================] - 43s 291ms/step - loss: 0.8759 - accuracy: 0.7282\n",
      "Epoch 96/300\n",
      "147/147 [==============================] - 43s 291ms/step - loss: 0.8676 - accuracy: 0.7313\n",
      "Epoch 97/300\n",
      "147/147 [==============================] - 43s 291ms/step - loss: 0.8584 - accuracy: 0.7357\n",
      "Epoch 98/300\n",
      "147/147 [==============================] - 44s 299ms/step - loss: 0.8449 - accuracy: 0.7406\n",
      "Epoch 99/300\n",
      "147/147 [==============================] - 44s 297ms/step - loss: 0.8310 - accuracy: 0.7450\n",
      "Epoch 100/300\n",
      "147/147 [==============================] - 43s 293ms/step - loss: 0.8193 - accuracy: 0.7482\n",
      "Epoch 101/300\n",
      "147/147 [==============================] - 43s 291ms/step - loss: 0.8071 - accuracy: 0.7530\n",
      "Epoch 102/300\n",
      "147/147 [==============================] - 43s 292ms/step - loss: 0.7935 - accuracy: 0.7585\n",
      "Epoch 103/300\n",
      "147/147 [==============================] - 46s 314ms/step - loss: 0.7883 - accuracy: 0.7595\n",
      "Epoch 104/300\n",
      "147/147 [==============================] - 45s 306ms/step - loss: 0.7820 - accuracy: 0.7603\n",
      "Epoch 105/300\n",
      "147/147 [==============================] - 44s 296ms/step - loss: 0.7758 - accuracy: 0.7635\n",
      "Epoch 106/300\n",
      "147/147 [==============================] - 44s 298ms/step - loss: 0.7672 - accuracy: 0.7656\n",
      "Epoch 107/300\n",
      "147/147 [==============================] - 44s 302ms/step - loss: 0.7627 - accuracy: 0.7667\n",
      "Epoch 108/300\n",
      "147/147 [==============================] - 43s 293ms/step - loss: 0.7556 - accuracy: 0.7701\n",
      "Epoch 109/300\n",
      "147/147 [==============================] - 42s 285ms/step - loss: 0.7486 - accuracy: 0.7718\n",
      "Epoch 110/300\n",
      "147/147 [==============================] - 41s 277ms/step - loss: 0.7400 - accuracy: 0.7759\n",
      "Epoch 111/300\n",
      "147/147 [==============================] - 41s 281ms/step - loss: 0.7261 - accuracy: 0.7809\n",
      "Epoch 112/300\n",
      "147/147 [==============================] - 42s 285ms/step - loss: 0.7143 - accuracy: 0.7848\n",
      "Epoch 113/300\n",
      "147/147 [==============================] - 42s 284ms/step - loss: 0.7048 - accuracy: 0.7890\n",
      "Epoch 114/300\n",
      "147/147 [==============================] - 41s 281ms/step - loss: 0.6982 - accuracy: 0.7920\n",
      "Epoch 115/300\n",
      "147/147 [==============================] - 41s 277ms/step - loss: 0.6916 - accuracy: 0.7944\n",
      "Epoch 116/300\n",
      "147/147 [==============================] - 41s 280ms/step - loss: 0.6855 - accuracy: 0.7958\n",
      "Epoch 117/300\n",
      "147/147 [==============================] - 41s 282ms/step - loss: 0.6792 - accuracy: 0.7987\n",
      "Epoch 118/300\n",
      "147/147 [==============================] - 41s 279ms/step - loss: 0.6753 - accuracy: 0.8002\n",
      "Epoch 119/300\n",
      "147/147 [==============================] - 42s 283ms/step - loss: 0.6660 - accuracy: 0.8028\n",
      "Epoch 120/300\n",
      "147/147 [==============================] - 42s 285ms/step - loss: 0.6600 - accuracy: 0.8057\n",
      "Epoch 121/300\n",
      "147/147 [==============================] - 41s 282ms/step - loss: 0.6504 - accuracy: 0.8101\n",
      "Epoch 122/300\n",
      "147/147 [==============================] - 43s 291ms/step - loss: 0.6413 - accuracy: 0.8141\n",
      "Epoch 123/300\n",
      "147/147 [==============================] - 43s 290ms/step - loss: 0.6337 - accuracy: 0.8167\n",
      "Epoch 124/300\n",
      "147/147 [==============================] - 42s 288ms/step - loss: 0.6268 - accuracy: 0.8206\n",
      "Epoch 125/300\n",
      "147/147 [==============================] - 45s 306ms/step - loss: 0.6194 - accuracy: 0.8234\n",
      "Epoch 126/300\n",
      "147/147 [==============================] - 44s 296ms/step - loss: 0.6158 - accuracy: 0.8248\n",
      "Epoch 127/300\n",
      "147/147 [==============================] - 43s 289ms/step - loss: 0.6102 - accuracy: 0.8265\n",
      "Epoch 128/300\n",
      "147/147 [==============================] - 42s 287ms/step - loss: 0.6066 - accuracy: 0.8277\n",
      "Epoch 129/300\n",
      "147/147 [==============================] - 42s 287ms/step - loss: 0.6059 - accuracy: 0.8274\n",
      "Epoch 130/300\n",
      "147/147 [==============================] - 43s 293ms/step - loss: 0.6030 - accuracy: 0.8287\n",
      "Epoch 131/300\n",
      "147/147 [==============================] - 41s 281ms/step - loss: 0.6020 - accuracy: 0.8270\n",
      "Epoch 132/300\n",
      "147/147 [==============================] - 41s 278ms/step - loss: 0.5962 - accuracy: 0.8302\n",
      "Epoch 133/300\n",
      "147/147 [==============================] - 41s 278ms/step - loss: 0.5923 - accuracy: 0.8313\n",
      "Epoch 134/300\n",
      "147/147 [==============================] - 217s 1s/step - loss: 0.5830 - accuracy: 0.8369\n",
      "Epoch 135/300\n",
      "147/147 [==============================] - 39s 265ms/step - loss: 0.5749 - accuracy: 0.8390\n",
      "Epoch 136/300\n",
      "147/147 [==============================] - 39s 264ms/step - loss: 0.5694 - accuracy: 0.8424\n",
      "Epoch 137/300\n",
      "147/147 [==============================] - 39s 268ms/step - loss: 0.5625 - accuracy: 0.8445\n",
      "Epoch 138/300\n",
      "147/147 [==============================] - 39s 266ms/step - loss: 0.5556 - accuracy: 0.8490\n",
      "Epoch 139/300\n",
      "147/147 [==============================] - 39s 266ms/step - loss: 0.5499 - accuracy: 0.8508\n",
      "Epoch 140/300\n",
      "147/147 [==============================] - 42s 285ms/step - loss: 0.5431 - accuracy: 0.8543\n",
      "Epoch 141/300\n",
      "147/147 [==============================] - 43s 290ms/step - loss: 0.5410 - accuracy: 0.8548\n",
      "Epoch 142/300\n",
      "147/147 [==============================] - 42s 285ms/step - loss: 0.5379 - accuracy: 0.8571\n",
      "Epoch 143/300\n",
      "147/147 [==============================] - 42s 285ms/step - loss: 0.5329 - accuracy: 0.8581\n",
      "Epoch 144/300\n",
      "147/147 [==============================] - 42s 285ms/step - loss: 0.5282 - accuracy: 0.8611\n",
      "Epoch 145/300\n",
      "147/147 [==============================] - 103s 702ms/step - loss: 0.5245 - accuracy: 0.8628\n",
      "Epoch 146/300\n",
      "147/147 [==============================] - 40s 273ms/step - loss: 0.5221 - accuracy: 0.8641\n",
      "Epoch 147/300\n",
      "147/147 [==============================] - 41s 279ms/step - loss: 0.5205 - accuracy: 0.8636\n",
      "Epoch 148/300\n",
      "147/147 [==============================] - 43s 290ms/step - loss: 0.5159 - accuracy: 0.8667\n",
      "Epoch 149/300\n",
      "147/147 [==============================] - 44s 302ms/step - loss: 0.5110 - accuracy: 0.8681\n",
      "Epoch 150/300\n",
      "147/147 [==============================] - 44s 296ms/step - loss: 0.5078 - accuracy: 0.8705\n",
      "Epoch 151/300\n",
      "147/147 [==============================] - 42s 289ms/step - loss: 0.5021 - accuracy: 0.8723\n",
      "Epoch 152/300\n",
      "147/147 [==============================] - 42s 285ms/step - loss: 0.4977 - accuracy: 0.8739\n",
      "Epoch 153/300\n",
      "147/147 [==============================] - 42s 284ms/step - loss: 0.4943 - accuracy: 0.8756\n",
      "Epoch 154/300\n",
      "147/147 [==============================] - 42s 284ms/step - loss: 0.4948 - accuracy: 0.8752\n",
      "Epoch 155/300\n",
      "147/147 [==============================] - 44s 301ms/step - loss: 0.4950 - accuracy: 0.8753\n",
      "Epoch 156/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 43s 295ms/step - loss: 0.4952 - accuracy: 0.8731\n",
      "Epoch 157/300\n",
      "147/147 [==============================] - 44s 300ms/step - loss: 0.4918 - accuracy: 0.8763\n",
      "Epoch 158/300\n",
      "147/147 [==============================] - 43s 292ms/step - loss: 0.4961 - accuracy: 0.8727\n",
      "Epoch 159/300\n",
      "147/147 [==============================] - 43s 293ms/step - loss: 0.4920 - accuracy: 0.8759\n",
      "Epoch 160/300\n",
      "147/147 [==============================] - 43s 290ms/step - loss: 0.4842 - accuracy: 0.8792\n",
      "Epoch 161/300\n",
      "147/147 [==============================] - 44s 297ms/step - loss: 0.4782 - accuracy: 0.8821\n",
      "Epoch 162/300\n",
      "147/147 [==============================] - 43s 292ms/step - loss: 0.4721 - accuracy: 0.8863\n",
      "Epoch 163/300\n",
      "147/147 [==============================] - 43s 293ms/step - loss: 0.4663 - accuracy: 0.8891\n",
      "Epoch 164/300\n",
      "147/147 [==============================] - 43s 292ms/step - loss: 0.4608 - accuracy: 0.8912\n",
      "Epoch 165/300\n",
      "147/147 [==============================] - 41s 282ms/step - loss: 0.4566 - accuracy: 0.8942\n",
      "Epoch 166/300\n",
      "147/147 [==============================] - 41s 279ms/step - loss: 0.4541 - accuracy: 0.8954\n",
      "Epoch 167/300\n",
      "147/147 [==============================] - 41s 279ms/step - loss: 0.4521 - accuracy: 0.8954\n",
      "Epoch 168/300\n",
      "147/147 [==============================] - 41s 281ms/step - loss: 0.4509 - accuracy: 0.8966\n",
      "Epoch 169/300\n",
      "147/147 [==============================] - 41s 282ms/step - loss: 0.4496 - accuracy: 0.8973\n",
      "Epoch 170/300\n",
      "147/147 [==============================] - 42s 283ms/step - loss: 0.4487 - accuracy: 0.8978\n",
      "Epoch 171/300\n",
      "147/147 [==============================] - 42s 284ms/step - loss: 0.4473 - accuracy: 0.8982\n",
      "Epoch 172/300\n",
      "147/147 [==============================] - 42s 284ms/step - loss: 0.4488 - accuracy: 0.8970\n",
      "Epoch 173/300\n",
      "147/147 [==============================] - 42s 285ms/step - loss: 0.4486 - accuracy: 0.8969\n",
      "Epoch 174/300\n",
      "147/147 [==============================] - 42s 285ms/step - loss: 0.4486 - accuracy: 0.8967\n",
      "Epoch 175/300\n",
      "147/147 [==============================] - 42s 286ms/step - loss: 0.4446 - accuracy: 0.8996\n",
      "Epoch 176/300\n",
      "147/147 [==============================] - 42s 287ms/step - loss: 0.4385 - accuracy: 0.9029\n",
      "Epoch 177/300\n",
      "147/147 [==============================] - 42s 287ms/step - loss: 0.4336 - accuracy: 0.9059\n",
      "Epoch 178/300\n",
      "147/147 [==============================] - 42s 288ms/step - loss: 0.4299 - accuracy: 0.9064\n",
      "Epoch 179/300\n",
      "147/147 [==============================] - 43s 295ms/step - loss: 0.4278 - accuracy: 0.9068\n",
      "Epoch 180/300\n",
      "147/147 [==============================] - 43s 290ms/step - loss: 0.4260 - accuracy: 0.9089\n",
      "Epoch 181/300\n",
      "147/147 [==============================] - 42s 289ms/step - loss: 0.4241 - accuracy: 0.9101\n",
      "Epoch 182/300\n",
      "147/147 [==============================] - 43s 290ms/step - loss: 0.4227 - accuracy: 0.9104\n",
      "Epoch 183/300\n",
      "147/147 [==============================] - 43s 292ms/step - loss: 0.4220 - accuracy: 0.9109\n",
      "Epoch 184/300\n",
      "147/147 [==============================] - 43s 291ms/step - loss: 0.4216 - accuracy: 0.9107\n",
      "Epoch 185/300\n",
      "147/147 [==============================] - 44s 299ms/step - loss: 0.4255 - accuracy: 0.9088\n",
      "Epoch 186/300\n",
      "147/147 [==============================] - 41s 280ms/step - loss: 0.4239 - accuracy: 0.9099\n",
      "Epoch 187/300\n",
      "147/147 [==============================] - 40s 275ms/step - loss: 0.4202 - accuracy: 0.9119\n",
      "Epoch 188/300\n",
      "147/147 [==============================] - 40s 273ms/step - loss: 0.4190 - accuracy: 0.9119\n",
      "Epoch 189/300\n",
      "147/147 [==============================] - 41s 278ms/step - loss: 0.4180 - accuracy: 0.9139\n",
      "Epoch 190/300\n",
      "147/147 [==============================] - 42s 288ms/step - loss: 0.4152 - accuracy: 0.9137\n",
      "Epoch 191/300\n",
      "147/147 [==============================] - 45s 303ms/step - loss: 0.4109 - accuracy: 0.9162\n",
      "Epoch 192/300\n",
      "147/147 [==============================] - 45s 309ms/step - loss: 0.4070 - accuracy: 0.9185\n",
      "Epoch 193/300\n",
      "147/147 [==============================] - 46s 312ms/step - loss: 0.4039 - accuracy: 0.9207\n",
      "Epoch 194/300\n",
      "147/147 [==============================] - 46s 310ms/step - loss: 0.4006 - accuracy: 0.9217\n",
      "Epoch 195/300\n",
      "147/147 [==============================] - 44s 300ms/step - loss: 0.3986 - accuracy: 0.9232\n",
      "Epoch 196/300\n",
      "147/147 [==============================] - 44s 297ms/step - loss: 0.3982 - accuracy: 0.9233\n",
      "Epoch 197/300\n",
      "147/147 [==============================] - 43s 293ms/step - loss: 0.3995 - accuracy: 0.9221\n",
      "Epoch 198/300\n",
      "147/147 [==============================] - 43s 289ms/step - loss: 0.4003 - accuracy: 0.9207\n",
      "Epoch 199/300\n",
      "147/147 [==============================] - 43s 291ms/step - loss: 0.4017 - accuracy: 0.9208\n",
      "Epoch 200/300\n",
      "147/147 [==============================] - 43s 289ms/step - loss: 0.4015 - accuracy: 0.9213\n",
      "Epoch 201/300\n",
      "147/147 [==============================] - 43s 290ms/step - loss: 0.4009 - accuracy: 0.9213\n",
      "Epoch 202/300\n",
      "147/147 [==============================] - 43s 294ms/step - loss: 0.3992 - accuracy: 0.9218\n",
      "Epoch 203/300\n",
      "147/147 [==============================] - 45s 308ms/step - loss: 0.3971 - accuracy: 0.9233\n",
      "Epoch 204/300\n",
      "147/147 [==============================] - 43s 295ms/step - loss: 0.3934 - accuracy: 0.9257\n",
      "Epoch 205/300\n",
      "147/147 [==============================] - 44s 296ms/step - loss: 0.3892 - accuracy: 0.9281\n",
      "Epoch 206/300\n",
      "147/147 [==============================] - 45s 303ms/step - loss: 0.3856 - accuracy: 0.9295\n",
      "Epoch 207/300\n",
      "147/147 [==============================] - 43s 293ms/step - loss: 0.3833 - accuracy: 0.9308\n",
      "Epoch 208/300\n",
      "147/147 [==============================] - 43s 295ms/step - loss: 0.3818 - accuracy: 0.9319\n",
      "Epoch 209/300\n",
      "147/147 [==============================] - 44s 298ms/step - loss: 0.3816 - accuracy: 0.9322\n",
      "Epoch 210/300\n",
      "147/147 [==============================] - 43s 295ms/step - loss: 0.3813 - accuracy: 0.9322\n",
      "Epoch 211/300\n",
      "147/147 [==============================] - 43s 294ms/step - loss: 0.3817 - accuracy: 0.9315\n",
      "Epoch 212/300\n",
      "147/147 [==============================] - 45s 303ms/step - loss: 0.3832 - accuracy: 0.9311\n",
      "Epoch 213/300\n",
      "147/147 [==============================] - 46s 314ms/step - loss: 0.3846 - accuracy: 0.9303\n",
      "Epoch 214/300\n",
      "147/147 [==============================] - 45s 306ms/step - loss: 0.3875 - accuracy: 0.9280\n",
      "Epoch 215/300\n",
      "147/147 [==============================] - 45s 309ms/step - loss: 0.3927 - accuracy: 0.9250\n",
      "Epoch 216/300\n",
      "147/147 [==============================] - 44s 302ms/step - loss: 0.3986 - accuracy: 0.9211\n",
      "Epoch 217/300\n",
      "147/147 [==============================] - 46s 310ms/step - loss: 0.4002 - accuracy: 0.9203\n",
      "Epoch 218/300\n",
      "147/147 [==============================] - 47s 316ms/step - loss: 0.3949 - accuracy: 0.9246\n",
      "Epoch 219/300\n",
      "147/147 [==============================] - 45s 308ms/step - loss: 0.3857 - accuracy: 0.9300\n",
      "Epoch 220/300\n",
      "147/147 [==============================] - 45s 309ms/step - loss: 0.3797 - accuracy: 0.9335\n",
      "Epoch 221/300\n",
      "147/147 [==============================] - 44s 302ms/step - loss: 0.3738 - accuracy: 0.9361\n",
      "Epoch 222/300\n",
      "147/147 [==============================] - 45s 309ms/step - loss: 0.3693 - accuracy: 0.9384\n",
      "Epoch 223/300\n",
      "147/147 [==============================] - 46s 310ms/step - loss: 0.3671 - accuracy: 0.9395\n",
      "Epoch 224/300\n",
      "147/147 [==============================] - 44s 301ms/step - loss: 0.3657 - accuracy: 0.9405\n",
      "Epoch 225/300\n",
      "147/147 [==============================] - 44s 300ms/step - loss: 0.3647 - accuracy: 0.9407\n",
      "Epoch 226/300\n",
      "147/147 [==============================] - 49s 333ms/step - loss: 0.3642 - accuracy: 0.9412\n",
      "Epoch 227/300\n",
      "147/147 [==============================] - 45s 305ms/step - loss: 0.3655 - accuracy: 0.9398\n",
      "Epoch 228/300\n",
      "147/147 [==============================] - 44s 298ms/step - loss: 0.3668 - accuracy: 0.9395\n",
      "Epoch 229/300\n",
      "147/147 [==============================] - 44s 298ms/step - loss: 0.3695 - accuracy: 0.9375\n",
      "Epoch 230/300\n",
      "147/147 [==============================] - 45s 308ms/step - loss: 0.3732 - accuracy: 0.9357\n",
      "Epoch 231/300\n",
      "147/147 [==============================] - 45s 306ms/step - loss: 0.3827 - accuracy: 0.9292\n",
      "Epoch 232/300\n",
      "147/147 [==============================] - 48s 327ms/step - loss: 0.3954 - accuracy: 0.9225\n",
      "Epoch 233/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 47s 319ms/step - loss: 0.4015 - accuracy: 0.9196\n",
      "Epoch 234/300\n",
      "147/147 [==============================] - 46s 310ms/step - loss: 0.3929 - accuracy: 0.9264\n",
      "Epoch 235/300\n",
      "147/147 [==============================] - 49s 331ms/step - loss: 0.3823 - accuracy: 0.9335\n",
      "Epoch 236/300\n",
      "147/147 [==============================] - 46s 314ms/step - loss: 0.3723 - accuracy: 0.9380\n",
      "Epoch 237/300\n",
      "147/147 [==============================] - 46s 311ms/step - loss: 0.3653 - accuracy: 0.9416\n",
      "Epoch 238/300\n",
      "147/147 [==============================] - 46s 310ms/step - loss: 0.3603 - accuracy: 0.9439\n",
      "Epoch 239/300\n",
      "147/147 [==============================] - 46s 310ms/step - loss: 0.3581 - accuracy: 0.9452\n",
      "Epoch 240/300\n",
      "147/147 [==============================] - 46s 312ms/step - loss: 0.3562 - accuracy: 0.9458\n",
      "Epoch 241/300\n",
      "147/147 [==============================] - 46s 312ms/step - loss: 0.3556 - accuracy: 0.9464\n",
      "Epoch 242/300\n",
      "147/147 [==============================] - 47s 318ms/step - loss: 0.3544 - accuracy: 0.9469\n",
      "Epoch 243/300\n",
      "147/147 [==============================] - 47s 316ms/step - loss: 0.3547 - accuracy: 0.9466\n",
      "Epoch 244/300\n",
      "147/147 [==============================] - 47s 317ms/step - loss: 0.3541 - accuracy: 0.9467\n",
      "Epoch 245/300\n",
      "147/147 [==============================] - 47s 317ms/step - loss: 0.3550 - accuracy: 0.9465\n",
      "Epoch 246/300\n",
      "147/147 [==============================] - 46s 315ms/step - loss: 0.3550 - accuracy: 0.9461\n",
      "Epoch 247/300\n",
      "147/147 [==============================] - 47s 319ms/step - loss: 0.3565 - accuracy: 0.9455\n",
      "Epoch 248/300\n",
      "147/147 [==============================] - 47s 319ms/step - loss: 0.3584 - accuracy: 0.9443\n",
      "Epoch 249/300\n",
      "147/147 [==============================] - 46s 312ms/step - loss: 0.3621 - accuracy: 0.9423\n",
      "Epoch 250/300\n",
      "147/147 [==============================] - 46s 313ms/step - loss: 0.3709 - accuracy: 0.9351\n",
      "Epoch 251/300\n",
      "147/147 [==============================] - 46s 313ms/step - loss: 0.3859 - accuracy: 0.9254\n",
      "Epoch 252/300\n",
      "147/147 [==============================] - 46s 310ms/step - loss: 0.3955 - accuracy: 0.9222\n",
      "Epoch 253/300\n",
      "147/147 [==============================] - 46s 314ms/step - loss: 0.3912 - accuracy: 0.9263\n",
      "Epoch 254/300\n",
      "147/147 [==============================] - 47s 317ms/step - loss: 0.3760 - accuracy: 0.9346\n",
      "Epoch 255/300\n",
      "147/147 [==============================] - 46s 314ms/step - loss: 0.3630 - accuracy: 0.9431\n",
      "Epoch 256/300\n",
      "147/147 [==============================] - 47s 319ms/step - loss: 0.3554 - accuracy: 0.9471\n",
      "Epoch 257/300\n",
      "147/147 [==============================] - 47s 318ms/step - loss: 0.3512 - accuracy: 0.9490\n",
      "Epoch 258/300\n",
      "147/147 [==============================] - 45s 304ms/step - loss: 0.3492 - accuracy: 0.9502\n",
      "Epoch 259/300\n",
      "147/147 [==============================] - 45s 303ms/step - loss: 0.3483 - accuracy: 0.9502\n",
      "Epoch 260/300\n",
      "147/147 [==============================] - 46s 313ms/step - loss: 0.3474 - accuracy: 0.9508\n",
      "Epoch 261/300\n",
      "147/147 [==============================] - 47s 317ms/step - loss: 0.3471 - accuracy: 0.9511\n",
      "Epoch 262/300\n",
      "147/147 [==============================] - 47s 322ms/step - loss: 0.3466 - accuracy: 0.9513\n",
      "Epoch 263/300\n",
      "147/147 [==============================] - 48s 328ms/step - loss: 0.3468 - accuracy: 0.9513\n",
      "Epoch 264/300\n",
      "147/147 [==============================] - 46s 313ms/step - loss: 0.3464 - accuracy: 0.9513\n",
      "Epoch 265/300\n",
      "147/147 [==============================] - 46s 314ms/step - loss: 0.3471 - accuracy: 0.9508\n",
      "Epoch 266/300\n",
      "147/147 [==============================] - 47s 318ms/step - loss: 0.3470 - accuracy: 0.9505\n",
      "Epoch 267/300\n",
      "147/147 [==============================] - 47s 316ms/step - loss: 0.3475 - accuracy: 0.9507\n",
      "Epoch 268/300\n",
      "147/147 [==============================] - 47s 316ms/step - loss: 0.3474 - accuracy: 0.9508\n",
      "Epoch 269/300\n",
      "147/147 [==============================] - 46s 315ms/step - loss: 0.3486 - accuracy: 0.9498\n",
      "Epoch 270/300\n",
      "147/147 [==============================] - 47s 317ms/step - loss: 0.3502 - accuracy: 0.9495\n",
      "Epoch 271/300\n",
      "147/147 [==============================] - 47s 317ms/step - loss: 0.3561 - accuracy: 0.9454\n",
      "Epoch 272/300\n",
      "147/147 [==============================] - 46s 315ms/step - loss: 0.3737 - accuracy: 0.9309\n",
      "Epoch 273/300\n",
      "147/147 [==============================] - 47s 317ms/step - loss: 0.4123 - accuracy: 0.9066\n",
      "Epoch 274/300\n",
      "147/147 [==============================] - 47s 317ms/step - loss: 0.4224 - accuracy: 0.9062\n",
      "Epoch 275/300\n",
      "147/147 [==============================] - 46s 316ms/step - loss: 0.3955 - accuracy: 0.9241\n",
      "Epoch 276/300\n",
      "147/147 [==============================] - 47s 318ms/step - loss: 0.3680 - accuracy: 0.9407\n",
      "Epoch 277/300\n",
      "147/147 [==============================] - 47s 320ms/step - loss: 0.3541 - accuracy: 0.9485\n",
      "Epoch 278/300\n",
      "147/147 [==============================] - 47s 318ms/step - loss: 0.3478 - accuracy: 0.9518\n",
      "Epoch 279/300\n",
      "147/147 [==============================] - 47s 320ms/step - loss: 0.3466 - accuracy: 0.9522\n",
      "Epoch 280/300\n",
      "147/147 [==============================] - 47s 317ms/step - loss: 0.3475 - accuracy: 0.9526\n",
      "Epoch 281/300\n",
      "147/147 [==============================] - 47s 317ms/step - loss: 0.3477 - accuracy: 0.9527\n",
      "Epoch 282/300\n",
      "147/147 [==============================] - 47s 318ms/step - loss: 0.3442 - accuracy: 0.9534\n",
      "Epoch 283/300\n",
      "147/147 [==============================] - 46s 316ms/step - loss: 0.3441 - accuracy: 0.9533\n",
      "Epoch 284/300\n",
      "147/147 [==============================] - 47s 317ms/step - loss: 0.3426 - accuracy: 0.9538\n",
      "Epoch 285/300\n",
      "147/147 [==============================] - 47s 320ms/step - loss: 0.3416 - accuracy: 0.9542\n",
      "Epoch 286/300\n",
      "147/147 [==============================] - 47s 321ms/step - loss: 0.3406 - accuracy: 0.9547\n",
      "Epoch 287/300\n",
      "147/147 [==============================] - 47s 322ms/step - loss: 0.3407 - accuracy: 0.9545\n",
      "Epoch 288/300\n",
      "147/147 [==============================] - 46s 315ms/step - loss: 0.3400 - accuracy: 0.9551\n",
      "Epoch 289/300\n",
      "147/147 [==============================] - 46s 312ms/step - loss: 0.3403 - accuracy: 0.9547\n",
      "Epoch 290/300\n",
      "147/147 [==============================] - 47s 319ms/step - loss: 0.3398 - accuracy: 0.9552\n",
      "Epoch 291/300\n",
      "147/147 [==============================] - 46s 315ms/step - loss: 0.3403 - accuracy: 0.9546\n",
      "Epoch 292/300\n",
      "147/147 [==============================] - 47s 320ms/step - loss: 0.3400 - accuracy: 0.9547\n",
      "Epoch 293/300\n",
      "147/147 [==============================] - 47s 318ms/step - loss: 0.3409 - accuracy: 0.9541\n",
      "Epoch 294/300\n",
      "147/147 [==============================] - 47s 317ms/step - loss: 0.3407 - accuracy: 0.9545\n",
      "Epoch 295/300\n",
      "147/147 [==============================] - 47s 319ms/step - loss: 0.3419 - accuracy: 0.9533\n",
      "Epoch 296/300\n",
      "147/147 [==============================] - 47s 322ms/step - loss: 0.3436 - accuracy: 0.9525\n",
      "Epoch 297/300\n",
      "147/147 [==============================] - 48s 323ms/step - loss: 0.3512 - accuracy: 0.9467\n",
      "Epoch 298/300\n",
      "147/147 [==============================] - 48s 326ms/step - loss: 0.3796 - accuracy: 0.9260\n",
      "Epoch 299/300\n",
      "147/147 [==============================] - 48s 326ms/step - loss: 0.4297 - accuracy: 0.8956\n",
      "Epoch 300/300\n",
      "147/147 [==============================] - 49s 331ms/step - loss: 0.4247 - accuracy: 0.9048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17a6bc040>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers, activations, models, preprocessing, utils\n",
    "import re\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "\n",
    "def tokenize(sentences):\n",
    "    tokens_list = []\n",
    "    vocabulary = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "        tokens = sentence.split()\n",
    "        vocabulary += tokens\n",
    "        tokens_list.append(tokens)\n",
    "    return tokens_list, vocabulary\n",
    "\n",
    "\n",
    "def data_generator(encoder_input_data, decoder_input_data, decoder_output_data, batch_size):\n",
    "    num_samples = encoder_input_data.shape[0]\n",
    "    steps_per_epoch = num_samples // batch_size\n",
    "\n",
    "    while True:\n",
    "        for step in range(steps_per_epoch):\n",
    "            batch_start = step * batch_size\n",
    "            batch_end = (step + 1) * batch_size\n",
    "\n",
    "            batch_encoder_input = encoder_input_data[batch_start:batch_end]\n",
    "            batch_decoder_input = decoder_input_data[batch_start:batch_end]\n",
    "            batch_decoder_output = decoder_output_data[batch_start:batch_end]\n",
    "\n",
    "            yield [batch_encoder_input, batch_decoder_input], batch_decoder_output\n",
    "\n",
    "\n",
    "dir_path = 'archive'\n",
    "files_list = os.listdir(dir_path + os.sep)\n",
    "questions, answers = [], []\n",
    "\n",
    "for filepath in files_list:\n",
    "    file_ = open(dir_path + os.sep + filepath, 'rb')\n",
    "    docs = yaml.safe_load(file_)\n",
    "    conversations = docs['conversations']\n",
    "    for con in conversations:\n",
    "        if len(con) > 2:\n",
    "            questions.append(con[0])\n",
    "            replies = con[1:]\n",
    "            ans = ''\n",
    "            for rep in replies:\n",
    "                ans += ' ' + rep\n",
    "            answers.append(ans)\n",
    "        elif len(con) > 1:\n",
    "            questions.append(con[0])\n",
    "            answers.append(con[1])\n",
    "\n",
    "answers_with_tags = []\n",
    "for i in range(len(answers)):\n",
    "    if type(answers[i]) == str:\n",
    "        answers_with_tags.append(answers[i])\n",
    "    else:\n",
    "        questions.pop(i)\n",
    "\n",
    "answers = []\n",
    "for i in range(len(answers_with_tags)):\n",
    "    answers.append('<START> ' + answers_with_tags[i] + ' <END>')\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(questions + answers)\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "\n",
    "vocab = []\n",
    "for word in tokenizer.word_index:\n",
    "    vocab.append(word)\n",
    "\n",
    "# encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
    "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
    "padded_questions = preprocessing.sequence.pad_sequences(tokenized_questions, maxlen=maxlen_questions, padding='post')\n",
    "encoder_input_data = np.array(padded_questions)\n",
    "\n",
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
    "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
    "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
    "decoder_input_data = np.array(padded_answers)\n",
    "\n",
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
    "for i in range(len(tokenized_answers)):\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
    "decoder_output_data = np.array(padded_answers)\n",
    "\n",
    "# Embedding, LSTM and Dense layers\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(maxlen_questions,))\n",
    "encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200, mask_zero=True)(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = tf.keras.layers.LSTM(200, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(maxlen_answers,))\n",
    "decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM(200, return_state=True, return_sequences=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE, activation=tf.keras.activations.softmax)\n",
    "output = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create the data generator\n",
    "train_generator = data_generator(encoder_input_data, decoder_input_data, decoder_output_data, batch_size)\n",
    "\n",
    "# Train the model using the generator\n",
    "steps_per_epoch = len(tokenized_questions) // batch_size\n",
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc999f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 16:22:10.929243: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2023-07-03 16:22:10.929261: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2023-07-03 16:22:10.929264: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2023-07-03 16:22:10.929291: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-07-03 16:22:10.929303: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 24)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 160)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 24, 200)              422200    ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 160, 200)             422200    ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 200),                320800    ['embedding[0][0]']           \n",
      "                              (None, 200),                                                        \n",
      "                              (None, 200)]                                                        \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, 160, 200),           320800    ['embedding_1[0][0]',         \n",
      "                              (None, 200),                           'lstm[0][1]',                \n",
      "                              (None, 200)]                           'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 160, 2111)            424311    ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1910311 (7.29 MB)\n",
      "Trainable params: 1910311 (7.29 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 16:22:12.694249: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-03 16:22:12.981582: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-03 16:22:13.114919: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\tfor Tuple type infernce function 0\n",
      "\twhile inferring type of node 'cond_19/output/_22'\n",
      "2023-07-03 16:22:13.117015: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-03 16:22:13.391977: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 1/13 [=>............................] - ETA: 27s - loss: 7.6550 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 16:22:13.805952: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 3s 78ms/step - loss: 7.5488 - accuracy: 0.0342\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 6.6921 - accuracy: 0.0370\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 1s 67ms/step - loss: 6.4661 - accuracy: 0.0494\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 6.3973 - accuracy: 0.0538\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 1s 66ms/step - loss: 6.3424 - accuracy: 0.0615\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 1s 63ms/step - loss: 6.3010 - accuracy: 0.0730\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 1s 65ms/step - loss: 6.2350 - accuracy: 0.0813\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 1s 63ms/step - loss: 6.1695 - accuracy: 0.0803\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 1s 65ms/step - loss: 6.0659 - accuracy: 0.0819\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 1s 65ms/step - loss: 5.9905 - accuracy: 0.0817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x292cc5a50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing, utils\n",
    "import re\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "    \n",
    "\n",
    "dir_path = 'archive'\n",
    "files_list = [file for file in os.listdir(dir_path) if not file.startswith('.DS_Store')]\n",
    "questions, answers = [], []\n",
    "\n",
    "for filepath in files_list:\n",
    "    file_ = open(dir_path + os.sep + filepath , 'rb')\n",
    "    docs = yaml.safe_load(file_)\n",
    "    conversations = docs['conversations']\n",
    "    for con in conversations:\n",
    "        if len(con) > 2 :\n",
    "            questions.append(con[0])\n",
    "            replies = con[1 :]\n",
    "            ans = ''\n",
    "            for rep in replies:\n",
    "                ans += ' ' + rep\n",
    "            answers.append(ans)\n",
    "        elif len(con)> 1:\n",
    "            questions.append(con[0])\n",
    "            answers.append(con[1])\n",
    "\n",
    "answers_with_tags = []\n",
    "for i in range(len(answers)):\n",
    "    if type(answers[i]) == str:\n",
    "        answers_with_tags.append(answers[i])\n",
    "    else:\n",
    "        questions.pop(i)\n",
    "\n",
    "answers = []\n",
    "for i in range(len(answers_with_tags)) :\n",
    "    answers.append('<START> ' + answers_with_tags[i] + ' <END>')\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(questions + answers)\n",
    "VOCAB_SIZE = len(tokenizer.word_index)+1\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "vocab = []\n",
    "for word in tokenizer.word_index:\n",
    "    vocab.append(word)\n",
    "\n",
    "def tokenize(sentences):\n",
    "    tokens_list = []\n",
    "    vocabulary = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "        tokens = sentence.split()\n",
    "        vocabulary += tokens\n",
    "        tokens_list.append(tokens)\n",
    "    return tokens_list , vocabulary\n",
    "\n",
    "# encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
    "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
    "padded_questions = preprocessing.sequence.pad_sequences(tokenized_questions , maxlen=maxlen_questions , padding='post')\n",
    "encoder_input_data = np.array(padded_questions)\n",
    "\n",
    "encoder_input_data.shape\n",
    "\n",
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
    "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
    "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers , maxlen=maxlen_answers , padding='post')\n",
    "decoder_input_data = np.array(padded_answers)\n",
    "\n",
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers , maxlen=maxlen_answers , padding='post')\n",
    "onehot_answers = utils.to_categorical(padded_answers , VOCAB_SIZE)\n",
    "decoder_output_data = np.array(onehot_answers)\n",
    "\n",
    "# Embedding, LSTM and Desne layers\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(maxlen_questions ,))\n",
    "encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM(200 , return_state=True)(encoder_embedding)\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(maxlen_answers , ))\n",
    "decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM(200 , return_state=True , return_sequences=True)\n",
    "decoder_outputs , _ , _ = decoder_lstm (decoder_embedding , initial_state=encoder_states)\n",
    "\n",
    "\n",
    "decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE , activation=tf.keras.activations.softmax) \n",
    "output = decoder_dense (decoder_outputs)\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "    \n",
    "\n",
    "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=16, epochs=10)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb062f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59ef5e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference():\n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=(200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=(200 ,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)\n",
    "    return encoder_model , decoder_model\n",
    "def preprocess_input(input_sentence):\n",
    "    tokens = re.sub('[^a-zA-Z]', ' ', input_sentence).lower().split()\n",
    "    # tokens = tokenize(input_sentence)\n",
    "    print(tokens)\n",
    "    tokens_list = []\n",
    "    for word in tokens:\n",
    "        tokens_list.append(tokenizer.word_index[word])\n",
    "    return preprocessing.sequence.pad_sequences([tokens_list] , maxlen=maxlen_questions , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "772a76ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: test—300epoch/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: test—300epoch/assets\n"
     ]
    }
   ],
   "source": [
    "enc_model , dec_model = inference()  \n",
    "model.save('test—300epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dcf605c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'python', 'programming', 'language', 'in', 'picsart', 'academy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 16:51:42.925848: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-08 16:51:43.045354: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 513ms/step\n",
      "1/1 [==============================] - 0s 386ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 16:51:43.413430: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-08 16:51:43.500324: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Human: What is Python programming language in Picsart Academy?\n",
      "\n",
      "Bot:  the only person you are destined to become is the person you decide to be ralph waldo emerson\n",
      "-------------------------\n",
      "['what', 'is', 'c']\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Human: What is C++?\n",
      "\n",
      "Bot:  the only person you are destined to become is the person you decide to be ralph waldo emerson\n",
      "-------------------------\n",
      "['in', 'which', 'parts', 'of', 'the', 'learning', 'model', 'at', 'picsart', 'academy', 'is', 'javascript', 'studied']\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Human: In which parts of the learning model at Picsart Academy is JavaScript studied\n",
      "\n",
      "Bot:  the only person you are destined to become is the person you decide to be ralph waldo emerson\n",
      "-------------------------\n",
      "['how', 'many', 'months', 'last', 'the', 'course', 'in', 'picsart', 'academy']\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Human: How many months last the course in Picsart Academy?\n",
      "\n",
      "Bot:  the only person you are destined to become is the person you decide to be ralph waldo emerson\n",
      "-------------------------\n",
      "['what', 'is', 'sandbox', 'at', 'picsart', 'academy']\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Human: What is Sandbox at Picsart Academy\n",
      "\n",
      "Bot:  the only person you are destined to become is the person you decide to be ralph waldo emerson\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "tests = ['What is Python programming language in Picsart Academy?', 'What is C++?', 'In which parts of the learning model at Picsart Academy is JavaScript studied',\n",
    "         'How many months last the course in Picsart Academy?', 'What is Sandbox at Picsart Academy', 'What courses there are in Level Up Bootcamp']\n",
    "\n",
    "for i in range(5):\n",
    "    states_values = enc_model.predict(preprocess_input(tests[i]))\n",
    "    empty_target_seq = np.zeros((1 , 1))\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    \n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
    "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
    "        sampled_word = None\n",
    "        \n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += f' {word}'\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros((1 , 1))  \n",
    "        empty_target_seq[0 , 0] = sampled_word_index\n",
    "        states_values = [h , c] \n",
    "    print(f'Human: {tests[i]}')\n",
    "    print()\n",
    "    decoded_translation = decoded_translation.split(' end')[0]\n",
    "    print(f'Bot: {decoded_translation}')\n",
    "    print('-'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7f34bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
